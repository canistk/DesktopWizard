using Newtonsoft.Json;
using Newtonsoft.Json.Converters;
namespace Deepseek
{
	public struct ChatCompletion
	{
		/// <summary>A unique identifier for the chat completion.</summary>
		[JsonProperty("id")]						public string		id;

		/// <summary>A list of chat completion choices.</summary>
		[JsonProperty("choices")]					public Choice[]		choices;

		/// <summary>The Unix timestamp (in seconds) of when the chat completion was created.</summary>
		[JsonProperty("created")]					public int			created;

		/// <summary>The model used for the chat completion.</summary>
		[JsonProperty("model")]						public string		model;

		/// <summary>The object type, which is always chat.completion.</summary>
		[JsonProperty("object")]					public string		obj;

		/// <summary>Usage statistics for the completion request.</summary>
		[JsonProperty("usage")]						public Usage		usage;

		/// <summary>This fingerprint represents the backend configuration that the model runs with.</summary>
		[JsonProperty("system_fingerprint")]		public string		systemFingerprint;
	}

	/// <summary>Usage statistics for the completion request.</summary>
	public struct Usage
	{
		/// <summary>Number of tokens in the generated completion.</summary>
		[JsonProperty("completion_tokens")]			public int completionTokens;

		/// <summary>Number of tokens in the prompt. It equals prompt_cache_hit_tokens + prompt_cache_miss_tokens.</summary>
		[JsonProperty("prompt_tokens")]				public int promptTokens;

		/// <summary>Number of tokens in the prompt that hits the context cache.</summary>
		[JsonProperty("prompt_cache_hit_tokens")]	public int promptCacheHitTokens;

		/// <summary>Number of tokens in the prompt that misses the context cache.</summary>
		[JsonProperty("prompt_cache_miss_tokens")]	public int promptCacheMissTokens;

		/// <summary>Total number of tokens used in the request (prompt + completion).</summary>
		[JsonProperty("total_tokens")]				public int totalTokens;

		[JsonProperty("prompt_tokens_details")]		public PromptTokensDetails promptTokensDetails;
	}

	/// <summary>Breakdown of tokens used in a completion</summary>
	public struct PromptTokensDetails
	{
		/// <summary>Tokens generated by the model for reasoning.</summary>
		[JsonProperty("cached_tokens", NullValueHandling = NullValueHandling.Ignore)] public int cachedTokens;
	}

	public struct Choice
	{
		[JsonProperty("index")]				public int				index;
		
		[JsonConverter(typeof(MessageConverter))]
		[JsonProperty("message", NullValueHandling = NullValueHandling.Ignore)]			public Message			message;
		
		[JsonProperty("delta", NullValueHandling = NullValueHandling.Ignore)]			public SteamContent		delta;

		[JsonProperty("logprobs", NullValueHandling = NullValueHandling.Ignore)]			public object			logprobs;

		/// <summary>
		/// Possible values
		/// stop, length, content_filter, tool_calls, insufficient_system_resource
		/// </summary>
		[JsonProperty("finish_reason", NullValueHandling = NullValueHandling.Ignore)]		public string			finishReason;
	}

	public class SteamContent
	{
		[JsonProperty("role")] public string role;
		[JsonProperty("content", NullValueHandling = NullValueHandling.Ignore)] public string content;
		[JsonProperty("tool_calls", NullValueHandling = NullValueHandling.Ignore)] public ToolCall[] toolCalls;
	}

	public class ToolCall
	{
		[JsonProperty("index")] public int index;
		[JsonProperty("id", NullValueHandling = NullValueHandling.Ignore)] public string id;
		[JsonProperty("type", NullValueHandling = NullValueHandling.Ignore)] public string type;
		[JsonProperty("function", NullValueHandling = NullValueHandling.Ignore)] public FunctionRequest function;
	}

	public class FunctionRequest
	{
		[JsonProperty("name")] public string name;
		[JsonProperty("arguments")] public string arguments;

		public override string ToString()
		{
			return $"{{{name}:Args={arguments}}}";
		}
	}
}